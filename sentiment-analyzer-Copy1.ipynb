{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "# Data manipulation/analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Text preprocessing/analysis\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context='talk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('depressedData_Edited.csv')\n",
    "#print(f\"{sample.shape[0]} rows and {sample.shape[1]} columns\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at splits 1 for depressed 0 for not depressed\n",
    "sample['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'sentiment140_processedv3.csv' \n",
    "\n",
    "import chardet\n",
    "with open(file, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "result\n",
    "\n",
    "\n",
    "df= pd.read_csv(file,encoding='ISO-8859-1')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the ones you want\n",
    "sample1 = sample[['new_tweets','sentiment']]\n",
    "\n",
    "# Encode to numeric\n",
    "sample1['target'] = np.where(sample['sentiment']=='depressed', 1, 0)\n",
    "# Check values\n",
    "sample1.groupby(['sentiment', 'target']).count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the ones you want\n",
    "df1 = df[['new_tweets','sentiment']]\n",
    "\n",
    "# Encode to numeric\n",
    "df1['target'] = np.where(df['sentiment']=='negative', 1, 0)\n",
    "# Check values\n",
    "df1.groupby(['sentiment', 'target']).count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample1['new_tweets'], sample['target'], test_size=6200, random_state=seed)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(f\"Train: {train.shape[0]} rows and {train.shape[1]} columns\")\n",
    "print(f\"{train['target'].value_counts()}\\n\")\n",
    "print(f\"Test: {test.shape[0]} rows and {test.shape[1]} columns\")\n",
    "print(test['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(df1['new_tweets'], df1['target'], test_size=112000, random_state=seed)\n",
    "\n",
    "train1 = pd.concat([X_train1, y_train1], axis=1)\n",
    "test1 = pd.concat([X_test1, y_test1], axis=1)\n",
    "\n",
    "print(f\"Train1: {train1.shape[0]} rows and {train1.shape[1]} columns\")\n",
    "print(f\"{train1['target'].value_counts()}\\n\")\n",
    "print(f\"Test1: {test1.shape[0]} rows and {test1.shape[1]} columns\")\n",
    "print(test1['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['dep', 'neu', 'nondep', 'compound']] = train['new_tweets'].apply(sid.polarity_scores).apply(pd.Series)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['pos', 'neg', 'neu', 'compound']:\n",
    "    plt.figure(figsize=(9,4))\n",
    "    sns.distplot(train.query(\"target==1\")[var], bins=30, kde=False, \n",
    "                 color='green', label='Positive')\n",
    "    sns.distplot(train.query(\"target==0\")[var], bins=30, kde=False, \n",
    "                 color='red', label='Negative')\n",
    "    plt.legend()\n",
    "    plt.title(f'Histogram of {var} by true sentiment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vader_polarity'] = np.where(train['pos']>train['neg'], 0, 1)\n",
    "target_names=['negative', 'positive']\n",
    "print(classification_report(train['target'], \n",
    "                            train['vader_polarity'], \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function so that we could reuse later\n",
    "def plot_cm(y_test, y_pred, target_names=['negative', 'positive'], \n",
    "            figsize=(5,3)):\n",
    "    \"\"\"Create a labelled confusion matrix plot.\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='BuGn', cbar=False, \n",
    "                ax=ax)\n",
    "    ax.set_title('Confusion matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_xticklabels(target_names)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_yticklabels(target_names, \n",
    "                       fontdict={'verticalalignment': 'center'});\n",
    "# Plot confusion matrix\n",
    "plot_cm(train['target'], train['vader_polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['vader_compound'] = np.where(train['compound']>0, 1, 0)\n",
    "print(classification_report(train['target'], \n",
    "                            train['vader_compound'], \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def create_baseline_models():\n",
    "    \"\"\"Create list of baseline models.\"\"\"\n",
    "    models = []\n",
    "    models.append(('log', LogisticRegression(random_state=seed, \n",
    "                                             max_iter=1000)))\n",
    "    models.append(('sgd', SGDClassifier(random_state=seed)))\n",
    "    models.append(('mnb', MultinomialNB()))\n",
    "    return models\n",
    "def assess(X, y, models, cv=5, scoring=['roc_auc', \n",
    "                                        'accuracy', \n",
    "                                        'f1']):\n",
    "    \"\"\"Provide summary of cross validation results for models.\"\"\"\n",
    "    results = pd.DataFrame()\n",
    "    for name, model in models:\n",
    "        result = pd.DataFrame(cross_validate(model, X, y, cv=cv, \n",
    "                                             scoring=scoring))\n",
    "        mean = result.mean().rename('{}_mean'.format)\n",
    "        std = result.std().rename('{}_std'.format)\n",
    "        results[name] = pd.concat([mean, std], axis=0)\n",
    "    return results.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = create_baseline_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "vectoriser = TfidfVectorizer(token_pattern=r'[a-z]+', \n",
    "                             stop_words='english', \n",
    "                             min_df=30, \n",
    "                             max_df=.7)\n",
    "X_train_simpler = vectoriser.fit_transform(X_train)\n",
    "# Assess the model\n",
    "assess(X_train_simpler, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function\n",
    "def preprocess_text(text):\n",
    "    # 1. Tokenise to alphabetic tokens\n",
    "    tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # 2. Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens = [lemmatiser.lemmatize(t.lower(), pos='v') \n",
    "              for t in tokens]\n",
    "    return tokens\n",
    "# Preprocess the data\n",
    "vectoriser = TfidfVectorizer(analyzer=preprocess_text, \n",
    "                             min_df=30, \n",
    "                             max_df=.7)\n",
    "X_train_simple = vectoriser.fit_transform(X_train)\n",
    "# Assess models\n",
    "assess(X_train_simple, y_train, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,5))\n",
    "columns = ['target', 'neg', 'neu', 'pos', 'compound']\n",
    "sns.heatmap(train[columns].corr(), annot=True, cmap='seismic_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a model\n",
    "sgd = SGDClassifier(random_state=seed)\n",
    "# Initialise a scaler\n",
    "scaler = MinMaxScaler()\n",
    "# Assess the model using scores\n",
    "scores = train[['neg', 'neu', 'pos', 'compound']]\n",
    "assess(scaler.fit_transform(scores), y_train, [('sgd', sgd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to training data\n",
    "sgd.fit(scores, y_train)\n",
    "# Get coefficients\n",
    "coefs = pd.DataFrame(data=sgd.coef_, columns=scores.columns).T\n",
    "coefs.rename(columns={0: 'coef'}, inplace=True)\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=coefs.index, y='coef', data=coefs)\n",
    "plt.title('Coefficients');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipe = Pipeline([('vectoriser', TfidfVectorizer(encoding='ISO-8859-1',token_pattern=r'[a-z]+', min_df=30, max_df=.6, ngram_range=(1,2))),\n",
    "                 ('model', SGDClassifier(random_state=seed, loss='log'))])\n",
    "pipe.fit(X_train.values.astype('U'),y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(pipe['model'].coef_, \n",
    "                     columns=pipe['vectoriser'].get_feature_names())\n",
    "coefs = coefs.T.rename(columns={0:'coef'}).sort_values('coef')\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipe.predict(X_train)\n",
    "print(classification_report(train_pred, \n",
    "                            y_train, \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipe.predict(X_test)\n",
    "print(classification_report(train_pred, \n",
    "                            y_test, \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipe.predict(X_train1.values.astype('U'))\n",
    "print(classification_report(train_pred, \n",
    "                            y_train1, \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(X_test1.values.astype('U'))\n",
    "print(classification_report(test_pred, \n",
    "                            y_test1, \n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
